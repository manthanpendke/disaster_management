# -*- coding: utf-8 -*-
"""major.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hxXD33gOOC57HnL6sdD2I2KtNYJK7gBm
"""



import os
import cv2
import numpy as np
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Flatten, Dense, Dropout
from tensorflow.keras.applications import VGG16
from tensorflow.keras.optimizers import Adam
from google.colab import drive
import zipfile

# Mount Google Drive
drive.mount('/content/drive')

# Define ZIP file and extraction path
zip_path = '/content/final.zip'  # Update if necessary
extract_path = '/content/final_dataset'

# Extract the ZIP file
if not os.path.exists(extract_path):
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        zip_ref.extractall(extract_path)
print("Dataset extracted successfully.")

# Path to dataset
base_path = extract_path

# Define mappings (Updated casualty_levels, danger_levels, authorities to 4 categories)
disaster_types = {"cyclone": 0, "earthquake": 1, "flood": 2, "wildfire": 3}
casualty_levels = {"low": 0, "medium": 1, "high": 2, "critical": 3}  # 4 categories
danger_levels = {"low": 0, "medium": 1, "high": 2, "extreme": 3}  # 4 categories
authorities = {"local": 0, "state": 1, "central": 2, "international": 3}  # 4 categories

# Define mappings for printing the category names
disaster_names = {0: "cyclone", 1: "earthquake", 2: "flood", 3: "wildfire"}
casualty_names = {0: "low", 1: "medium", 2: "high", 3: "critical"}
danger_names = {0: "low", 1: "medium", 2: "high", 3: "extreme"}
authority_names = {0: "local", 1: "state", 2: "central", 3: "international"}

# Function to load dataset
def load_dataset(base_path, split, target_size=(224, 224)):
    images = []
    labels_disaster = []
    labels_casualty = []
    labels_danger = []
    labels_authority = []

    split_path = os.path.join(base_path, split)
    if not os.path.exists(split_path):
        raise ValueError(f"Expected split folder not found: {split_path}")

    for disaster_type in os.listdir(split_path):
        disaster_path = os.path.join(split_path, disaster_type)
        if os.path.isdir(disaster_path):
            for img_name in os.listdir(disaster_path):
                img_path = os.path.join(disaster_path, img_name)
                if img_path.endswith('.jpg') or img_path.endswith('.png'):
                    img = cv2.imread(img_path)
                    if img is not None:
                        img = cv2.resize(img, target_size) / 255.0
                        images.append(img)

                        # Assign labels (example logic, can be customized)
                        labels_disaster.append(disaster_types[disaster_type])
                        labels_casualty.append(casualty_levels["medium"])  # Placeholder logic
                        labels_danger.append(danger_levels["medium"])  # Placeholder logic
                        labels_authority.append(authorities["state"])  # Placeholder logic

    images = np.array(images)
    labels_disaster = to_categorical(labels_disaster, num_classes=len(disaster_types))
    labels_casualty = to_categorical(labels_casualty, num_classes=len(casualty_levels))  # 4 categories
    labels_danger = to_categorical(labels_danger, num_classes=len(danger_levels))  # 4 categories
    labels_authority = to_categorical(labels_authority, num_classes=len(authorities))  # 4 categories

    return images, labels_disaster, labels_casualty, labels_danger, labels_authority

# Load datasets
def load_all_data(base_path):
    print("Loading training data...")
    X_train, y_disaster_train, y_casualty_train, y_danger_train, y_authority_train = load_dataset(
        base_path, "train"
    )
    print("Loading validation data...")
    X_val, y_disaster_val, y_casualty_val, y_danger_val, y_authority_val = load_dataset(
        base_path, "validation"
    )
    print("Loading test data...")
    X_test, y_disaster_test, y_casualty_test, y_danger_test, y_authority_test = load_dataset(
        base_path, "test"
    )
    return (
        X_train, y_disaster_train, y_casualty_train, y_danger_train, y_authority_train,
        X_val, y_disaster_val, y_casualty_val, y_danger_val, y_authority_val,
        X_test, y_disaster_test, y_casualty_test, y_danger_test, y_authority_test
    )

# Load all data
(
    X_train, y_disaster_train, y_casualty_train, y_danger_train, y_authority_train,
    X_val, y_disaster_val, y_casualty_val, y_danger_val, y_authority_val,
    X_test, y_disaster_test, y_casualty_test, y_danger_test, y_authority_test
) = load_all_data(base_path)

# Build the model
base_model = VGG16(weights="imagenet", include_top=False, input_shape=(224, 224, 3))
for layer in base_model.layers:
    layer.trainable = False

x = Flatten()(base_model.output)
x = Dense(512, activation="relu")(x)
x = Dropout(0.5)(x)

disaster_output = Dense(len(disaster_types), activation="softmax", name="disaster")(x)
casualty_output = Dense(len(casualty_levels), activation="softmax", name="casualty")(x)  # 4 categories
danger_output = Dense(len(danger_levels), activation="softmax", name="danger")(x)  # 4 categories
authority_output = Dense(len(authorities), activation="softmax", name="authority")(x)  # 4 categories

model = Model(inputs=base_model.input, outputs=[disaster_output, casualty_output, danger_output, authority_output])

# Compile the model with metrics for each output
model.compile(
    optimizer=Adam(),
    loss={
        "disaster": "categorical_crossentropy",
        "casualty": "categorical_crossentropy",
        "danger": "categorical_crossentropy",
        "authority": "categorical_crossentropy",
    },
    metrics={
        "disaster": ["accuracy"],
        "casualty": ["accuracy"],
        "danger": ["accuracy"],
        "authority": ["accuracy"],
    }
)

# Train the model
history = model.fit(
    X_train,
    {
        "disaster": y_disaster_train,
        "casualty": y_casualty_train,
        "danger": y_danger_train,
        "authority": y_authority_train,
    },
    validation_data=(
        X_val,
        {
            "disaster": y_disaster_val,
            "casualty": y_casualty_val,
            "danger": y_danger_val,
            "authority": y_authority_val,
        },
    ),
    epochs=10,
    batch_size=32,
)

# Evaluate the model
test_results = model.evaluate(
    X_test,
    {
        "disaster": y_disaster_test,
        "casualty": y_casualty_test,
        "danger": y_danger_test,
        "authority": y_authority_test,
    }
)
print("Test Results:", test_results)

# Predict on a test sample
sample_image = X_test[0:1]
predictions = model.predict(sample_image)

print("Predictions:")
print("Disaster:", disaster_names[np.argmax(predictions[0])])
print("Casualty:", casualty_names[np.argmax(predictions[1])])
print("Danger:", danger_names[np.argmax(predictions[2])])
print("Authority:", authority_names[np.argmax(predictions[3])])